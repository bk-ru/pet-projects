{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "d223b5aa-049d-4676-b70d-3c51e39083ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7de3a8b616d0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_pYr7PZi2Sn"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 2)\n",
        "    )\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaJXwt4Mg6UD",
        "outputId": "d08fa1b1-a76a-4d39-f8ac-0892a242e441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "\n",
        "    with torch.no_grad():\n",
        "      tensor_ = torch.from_numpy(states).float()\n",
        "      logits = model(tensor_)\n",
        "      probs_tensor = torch.softmax(logits, dim=-1)\n",
        "      probs = probs_tensor.numpy()\n",
        "\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states, model)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]), model)[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sdENWJAi2Sz"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    cumulative_rewards = []\n",
        "    G_next = 0\n",
        "\n",
        "    for i, reward in enumerate(reversed(rewards)):\n",
        "      current_sum = reward + gamma * G_next\n",
        "      G_next = current_sum\n",
        "      cumulative_rewards.insert(0, current_sum)\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DX39wcUi2S3",
        "outputId": "445cd5aa-af07-4f45-fc6d-7ec4f26a2d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33CMbwYrg6UG"
      },
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8vFM5O_g6UH"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, actions, rewards, n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    logits: torch.Tensor shape [T, n_actions]\n",
        "    actions: list or array of ints length T\n",
        "    rewards: list of immediate rewards length T\n",
        "    returns: scalar torch.Tensor loss\n",
        "    \"\"\"\n",
        "    # convert\n",
        "    actions_t = torch.tensor(actions, dtype=torch.long)\n",
        "    returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "    # compute probabilities and log-probs (torch tensors)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    log_probs = torch.log_softmax(logits, dim=1)\n",
        "\n",
        "    # select log-prob for each taken action\n",
        "    # gather expects dims: (batch,1) -> then squeeze to [T]\n",
        "    log_probs_for_actions = log_probs.gather(1, actions_t.view(-1, 1)).squeeze(1)\n",
        "\n",
        "    # J_hat = (1/N) * sum_t log_pi(a_t|s_t) * G_t\n",
        "    J_hat = torch.mean(log_probs_for_actions * returns_t)\n",
        "\n",
        "    # entropy: per timestep H = - sum_a p(a) log p(a); use mean entropy\n",
        "    entropy_per_t = -torch.sum(probs * log_probs, dim=1)\n",
        "    entropy = torch.mean(entropy_per_t)\n",
        "\n",
        "    # loss = -J_hat - entropy_coef * entropy\n",
        "    loss = -J_hat - entropy_coef * entropy\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckHj5sXBi2TE",
        "outputId": "1a57a815-0c16-44f5-99ea-56208471ea30",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2359954175.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean reward:21.550\n",
            "mean reward:20.980\n",
            "mean reward:22.630\n",
            "mean reward:29.420\n",
            "mean reward:32.990\n",
            "mean reward:38.250\n",
            "mean reward:41.790\n",
            "mean reward:58.880\n",
            "mean reward:69.500\n",
            "mean reward:99.720\n",
            "mean reward:141.130\n",
            "mean reward:160.080\n",
            "mean reward:177.890\n",
            "mean reward:141.620\n",
            "mean reward:172.970\n",
            "mean reward:251.690\n",
            "mean reward:296.640\n",
            "mean reward:363.600\n",
            "mean reward:283.180\n",
            "mean reward:153.590\n",
            "mean reward:234.960\n",
            "mean reward:185.900\n",
            "mean reward:184.780\n",
            "mean reward:307.200\n",
            "mean reward:528.310\n",
            "mean reward:538.880\n",
            "mean reward:357.440\n",
            "mean reward:425.240\n",
            "mean reward:528.620\n",
            "mean reward:295.780\n",
            "mean reward:183.650\n",
            "mean reward:373.710\n",
            "mean reward:383.980\n",
            "mean reward:160.760\n",
            "mean reward:147.200\n",
            "mean reward:197.350\n",
            "mean reward:187.670\n",
            "mean reward:143.950\n",
            "mean reward:167.080\n",
            "mean reward:141.700\n",
            "mean reward:197.590\n",
            "mean reward:147.560\n",
            "mean reward:137.520\n",
            "mean reward:120.080\n",
            "mean reward:138.310\n",
            "mean reward:173.920\n",
            "mean reward:365.010\n",
            "mean reward:213.910\n",
            "mean reward:365.660\n",
            "mean reward:183.300\n",
            "mean reward:144.360\n",
            "mean reward:243.420\n",
            "mean reward:124.250\n",
            "mean reward:108.230\n",
            "mean reward:141.450\n",
            "mean reward:408.810\n",
            "mean reward:299.700\n",
            "mean reward:215.850\n",
            "mean reward:144.600\n",
            "mean reward:124.270\n",
            "mean reward:250.450\n",
            "mean reward:201.070\n",
            "mean reward:107.630\n",
            "mean reward:142.380\n",
            "mean reward:146.050\n",
            "mean reward:176.390\n",
            "mean reward:246.570\n",
            "mean reward:190.090\n",
            "mean reward:245.450\n",
            "mean reward:204.970\n",
            "mean reward:191.940\n",
            "mean reward:190.910\n",
            "mean reward:171.480\n",
            "mean reward:117.410\n",
            "mean reward:163.060\n",
            "mean reward:138.720\n",
            "mean reward:167.060\n",
            "mean reward:152.320\n",
            "mean reward:134.530\n",
            "mean reward:184.760\n",
            "mean reward:117.100\n",
            "mean reward:142.180\n",
            "mean reward:148.840\n",
            "mean reward:99.850\n",
            "mean reward:99.370\n",
            "mean reward:169.110\n",
            "mean reward:111.740\n",
            "mean reward:116.230\n",
            "mean reward:163.410\n",
            "mean reward:255.620\n",
            "mean reward:164.390\n",
            "mean reward:183.720\n",
            "mean reward:136.510\n",
            "mean reward:117.920\n",
            "mean reward:122.980\n",
            "mean reward:118.940\n",
            "mean reward:118.270\n",
            "mean reward:130.460\n",
            "mean reward:156.420\n",
            "mean reward:129.940\n",
            "mean reward:103.150\n",
            "mean reward:156.820\n",
            "mean reward:143.500\n",
            "mean reward:165.970\n",
            "mean reward:182.570\n",
            "mean reward:212.960\n",
            "mean reward:269.440\n",
            "mean reward:302.330\n",
            "mean reward:232.350\n",
            "mean reward:270.770\n",
            "mean reward:189.940\n",
            "mean reward:133.550\n",
            "mean reward:137.340\n",
            "mean reward:127.290\n",
            "mean reward:122.620\n",
            "mean reward:119.570\n",
            "mean reward:112.370\n",
            "mean reward:118.890\n",
            "mean reward:121.030\n",
            "mean reward:121.410\n",
            "mean reward:123.330\n",
            "mean reward:126.110\n",
            "mean reward:115.730\n",
            "mean reward:137.910\n",
            "mean reward:183.720\n",
            "mean reward:147.980\n",
            "mean reward:125.640\n",
            "mean reward:102.090\n",
            "mean reward:98.690\n",
            "mean reward:117.440\n",
            "mean reward:104.190\n",
            "mean reward:102.900\n",
            "mean reward:98.880\n",
            "mean reward:107.520\n",
            "mean reward:114.350\n",
            "mean reward:117.640\n",
            "mean reward:115.370\n",
            "mean reward:117.130\n",
            "mean reward:119.240\n",
            "mean reward:123.800\n",
            "mean reward:125.780\n",
            "mean reward:175.020\n",
            "mean reward:194.590\n",
            "mean reward:134.010\n",
            "mean reward:127.840\n",
            "mean reward:120.720\n",
            "mean reward:191.150\n",
            "mean reward:143.750\n",
            "mean reward:126.750\n",
            "mean reward:124.090\n",
            "mean reward:166.640\n",
            "mean reward:142.290\n",
            "mean reward:144.820\n",
            "mean reward:264.580\n",
            "mean reward:158.070\n",
            "mean reward:128.780\n",
            "mean reward:116.700\n",
            "mean reward:125.300\n",
            "mean reward:126.950\n",
            "mean reward:122.470\n",
            "mean reward:140.830\n",
            "mean reward:177.470\n",
            "mean reward:116.540\n",
            "mean reward:102.950\n",
            "mean reward:103.120\n",
            "mean reward:112.270\n",
            "mean reward:123.790\n",
            "mean reward:241.050\n",
            "mean reward:173.540\n",
            "mean reward:137.680\n",
            "mean reward:128.850\n",
            "mean reward:137.990\n",
            "mean reward:123.650\n",
            "mean reward:119.750\n",
            "mean reward:114.270\n",
            "mean reward:109.010\n",
            "mean reward:103.830\n",
            "mean reward:104.950\n",
            "mean reward:109.940\n",
            "mean reward:109.940\n",
            "mean reward:113.180\n",
            "mean reward:119.000\n",
            "mean reward:119.080\n",
            "mean reward:105.480\n",
            "mean reward:114.510\n",
            "mean reward:113.930\n",
            "mean reward:126.730\n",
            "mean reward:128.100\n",
            "mean reward:123.670\n",
            "mean reward:126.810\n",
            "mean reward:118.700\n",
            "mean reward:123.210\n",
            "mean reward:123.020\n",
            "mean reward:126.020\n",
            "mean reward:121.300\n",
            "mean reward:117.740\n",
            "mean reward:121.280\n",
            "mean reward:126.990\n",
            "mean reward:126.520\n",
            "mean reward:124.610\n",
            "mean reward:114.330\n",
            "mean reward:114.840\n",
            "mean reward:119.750\n",
            "mean reward:129.480\n",
            "mean reward:131.580\n",
            "mean reward:167.020\n",
            "mean reward:168.290\n",
            "mean reward:129.280\n",
            "mean reward:122.750\n",
            "mean reward:129.130\n",
            "mean reward:136.130\n",
            "mean reward:123.370\n",
            "mean reward:122.510\n",
            "mean reward:119.480\n",
            "mean reward:117.380\n",
            "mean reward:124.880\n",
            "mean reward:126.200\n",
            "mean reward:171.380\n",
            "mean reward:242.320\n",
            "mean reward:120.280\n",
            "mean reward:134.080\n",
            "mean reward:199.230\n",
            "mean reward:205.140\n",
            "mean reward:121.600\n",
            "mean reward:163.150\n",
            "mean reward:103.780\n",
            "mean reward:102.160\n",
            "mean reward:114.550\n",
            "mean reward:182.730\n",
            "mean reward:134.010\n",
            "mean reward:174.040\n",
            "mean reward:99.460\n",
            "mean reward:97.620\n",
            "mean reward:118.970\n",
            "mean reward:120.970\n",
            "mean reward:156.080\n",
            "mean reward:255.360\n",
            "mean reward:298.230\n",
            "mean reward:157.780\n",
            "mean reward:291.230\n",
            "mean reward:228.700\n",
            "mean reward:131.320\n",
            "mean reward:136.820\n",
            "mean reward:212.630\n",
            "mean reward:130.690\n",
            "mean reward:100.040\n",
            "mean reward:108.260\n",
            "mean reward:122.820\n",
            "mean reward:99.730\n",
            "mean reward:99.580\n",
            "mean reward:97.130\n",
            "mean reward:105.620\n",
            "mean reward:103.970\n",
            "mean reward:101.480\n",
            "mean reward:109.510\n",
            "mean reward:110.940\n",
            "mean reward:116.300\n",
            "mean reward:112.340\n",
            "mean reward:112.720\n",
            "mean reward:119.550\n",
            "mean reward:122.770\n",
            "mean reward:117.350\n",
            "mean reward:111.630\n",
            "mean reward:105.480\n",
            "mean reward:95.650\n",
            "mean reward:100.740\n",
            "mean reward:100.930\n",
            "mean reward:92.160\n",
            "mean reward:96.500\n",
            "mean reward:104.190\n",
            "mean reward:103.440\n",
            "mean reward:102.450\n",
            "mean reward:94.770\n",
            "mean reward:103.120\n",
            "mean reward:96.610\n",
            "mean reward:105.410\n",
            "mean reward:113.330\n",
            "mean reward:112.990\n",
            "mean reward:114.380\n",
            "mean reward:116.720\n",
            "mean reward:117.550\n",
            "mean reward:119.310\n",
            "mean reward:133.790\n",
            "mean reward:191.570\n",
            "mean reward:770.120\n",
            "mean reward:739.040\n",
            "mean reward:974.410\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2qfwO4Gg6UI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "n_actions = env_for_video.action_space.n\n",
        "\n",
        "episode_index = 0\n",
        "step_starting_index = 0\n",
        "\n",
        "obs, info = env_for_video.reset()\n",
        "\n",
        "for step_index in range(800):\n",
        "    probs = predict_probs(np.array([obs]), model)[0]\n",
        "    action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done or step_index == 799:\n",
        "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "        frames = env_for_video.render()\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        save_video(\n",
        "            frames, \"videos\",\n",
        "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "            step_starting_index=step_starting_index,\n",
        "            episode_index=episode_index,\n",
        "        )\n",
        "        episode_index += 1\n",
        "        step_starting_index = step_index + 1\n",
        "        obs, info = env_for_video.reset()\n",
        "\n",
        "env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjTLj058g6UI"
      },
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IoPvSWtg6UI"
      },
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ycjt6crxg6UI",
        "outputId": "529fc1ed-edbf-4f75-803d-d2b04a84558c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7de29e334620>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXB55Kc_g6UJ"
      },
      "outputs": [],
      "source": [
        "# Your brave and victorious code here."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
